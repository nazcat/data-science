{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4832c738",
   "metadata": {},
   "source": [
    "# Classification Model using Parameters from Ad URLs\n",
    "Resource: https://docs.google.com/spreadsheets/d/12f6zRQyWlPnVTHjkJwY2jZSjVVAkpaIa/edit#gid=1812140099"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b64b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uremekn\\Anaconda3\\lib\\site-packages\\snowflake\\connector\\options.py:94: UserWarning: You have an incompatible version of 'pyarrow' installed (6.0.1), please install a version that adheres to: 'pyarrow<3.1.0,>=3.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "from snowflake.connector import connect\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e9516",
   "metadata": {},
   "source": [
    "### Load the utm source/medium actual and ideal datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c08dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = snowflake.connector.connect(\n",
    "        user='<USER>',\n",
    "        password='<PASS>',\n",
    "        account='<ACCOUNT>',\n",
    "        warehouse='<WAREHOUSE>',\n",
    "        database='<DATABASE>',\n",
    "        schema='<SCHEMA>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e6f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_actual = \"\"\" select \n",
    "        distinct utm_sid\n",
    "            , utm_source\n",
    "            , utm_medium\n",
    "            , sum(mau) as mau\n",
    "\n",
    "        from (\n",
    "          select\n",
    "            distinct utm_concat\n",
    "            , utm_sid\n",
    "            , utm_source\n",
    "            , utm_medium\n",
    "            , count(client_sid) as mau\n",
    "\n",
    "            from (\n",
    "                select\n",
    "                    distinct a.client_sid\n",
    "                    , u.utm_sid\n",
    "                    , u.utm_source\n",
    "                    , u.utm_medium\n",
    "                    , concat(u.utm_source,'_',u.utm_medium) as utm_concat\n",
    "                    from ODIN_PRD.DW_ODIN.UTM_DIM u\n",
    "                    join ODIN_PRD.RPT.HOURLY_TVS_AGG a on u.utm_sid = a.utm_sid\n",
    "                    join ODIN_PRD.DW_ODIN.HOUR_DIM h on h.hour_sid = a.hour_sid\n",
    "\n",
    "              union\n",
    "\n",
    "                select\n",
    "                    distinct a.client_sid\n",
    "                    , u.utm_sid\n",
    "                    , u.utm_source\n",
    "                    , u.utm_medium\n",
    "                    , concat(u.utm_source,'_',u.utm_medium) as utm_concat\n",
    "                    from ODIN_PRD.DW_ODIN.S_UTM_DIM u\n",
    "                    join ODIN_PRD.RPT.S_HOURLY_INACTIVE_TVS_AGG a on u.utm_sid = a.utm_sid\n",
    "                    join ODIN_PRD.DW_ODIN.HOUR_DIM h on h.hour_sid = a.hour_sid\n",
    "                    )\n",
    "\n",
    "            group by 1,2,3,4\n",
    "            )\n",
    "\n",
    "        group by 1,2,3; \"\"\"\n",
    "\n",
    "actual = pd.read_sql_query(sql_actual, con=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecf201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data for source and medium ideal values\n",
    "sql_ideal = \"\"\" \n",
    "    SELECT \n",
    "        SOURCE_IDEAL\n",
    "        , MEDIUM_IDEAL\n",
    "        , CHANNEL\n",
    "        , ACQUISITION\n",
    "        , BRAND\n",
    "        , CONTENT\n",
    "        , CRM\n",
    "        , DISTRIBUTION\n",
    "        , OTHER\n",
    "        , UNDEFINED\n",
    "        , ORGANIC_PAID\n",
    "        \n",
    "    FROM SANDBOX.ANALYSIS_MARKETING.UTM_MAPPING; \"\"\"\n",
    "\n",
    "ideal = pd.read_sql_query(sql_ideal, con=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6305f",
   "metadata": {},
   "source": [
    "### Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae0a1cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-b9a2305f4dac>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_filtered[utm_actual] = actual_filtered[utm_actual].astype(str).apply(str.lower).replace({'nan':'na','none':'na'})\n",
      "<ipython-input-5-b9a2305f4dac>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_filtered[utm_actual] = actual_filtered[utm_actual].astype(str).apply(str.lower).replace({'nan':'na','none':'na'})\n"
     ]
    }
   ],
   "source": [
    "def similarity(utm_actual, utm_ideal):\n",
    "    \n",
    "    # filter data to utms with maus > 100\n",
    "    actual_filtered = actual[actual['MAU'] > 100]\n",
    "    actual_filtered[utm_actual] = actual_filtered[utm_actual].astype(str).apply(str.lower).replace({'nan':'na','none':'na'})  \n",
    "\n",
    "    # pull actual and ideal source and medium lists to prep for cosine similarity calculation\n",
    "    ideal_list = list(set(ideal[utm_ideal].dropna()))\n",
    "    actual_list = list(set(actual_filtered[utm_actual].dropna()))\n",
    "    \n",
    "    # cosine similarity for utm source\n",
    "    def word2vec(word):\n",
    "        from collections import Counter\n",
    "        from math import sqrt\n",
    "\n",
    "        # count the characters in word\n",
    "        cw = Counter(word)\n",
    "        # precomputes a set of the different characters\n",
    "        sw = set(cw)\n",
    "        # precomputes the \"length\" of the word vector\n",
    "        lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "        # return a tuple\n",
    "        return cw, sw, lw\n",
    "\n",
    "    def cosdis(v1, v2):\n",
    "        # which characters are common to the two words?\n",
    "        common = v1[1].intersection(v2[1])\n",
    "        # by definition of cosine distance we have\n",
    "        if v1[2] > 0 and v2[2] > 0:\n",
    "            return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "    threshold = 0.0     # if needed\n",
    "\n",
    "    similarity = []\n",
    "    for key in actual_list:\n",
    "        for word in ideal_list:\n",
    "            try:\n",
    "                # print(key)\n",
    "                # print(word)\n",
    "                res = cosdis(word2vec(word), word2vec(key))\n",
    "                # print(res)\n",
    "                similarity.append([word, key, res])\n",
    "                #print(\"The cosine similarity between : {} and : {} is: {}\".format(word, key, res*100))\n",
    "                # if res > threshold:\n",
    "                #     print(\"Found a word with cosine distance > 80 : {} with original word: {}\".format(word, key))\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(similarity, columns=[utm_actual[4:] + '_IDEAL', utm_actual[4:] + '_ACTUAL', utm_actual[4:] + '_SIMILARITY_SCORE'])\n",
    "    df.sort_values(utm_actual[4:] + '_SIMILARITY_SCORE', ascending=False, inplace=True)\n",
    "    \n",
    "    df_final = pd.merge(df, actual_filtered['UTM_SID'], how='left', left_on=df[utm_actual[4:] + '_ACTUAL'], right_on=actual_filtered[utm_actual])\n",
    "    del df_final['key_0']\n",
    "\n",
    "    df_final.sort_values(utm_actual[4:] + '_SIMILARITY_SCORE', ascending=False)\n",
    "    df_final = df_final[df_final[utm_actual[4:] + '_SIMILARITY_SCORE'] >= 0.9]\n",
    "\n",
    "    return df_final\n",
    "        \n",
    "source_df = similarity('UTM_SOURCE', 'SOURCE_IDEAL')\n",
    "medium_df = similarity('UTM_MEDIUM', 'MEDIUM_IDEAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a44d59",
   "metadata": {},
   "source": [
    "### Merge Source and Medium datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "709b8fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-69da4ebfd24d>:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final['UTM_SID'] = final[['UTM_SID_x', 'UTM_SID_y']].bfill(axis=1).iloc[:, 0]\n",
      "<ipython-input-6-69da4ebfd24d>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final['UTM_SID'] = final['UTM_SID'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "# merge source and medium datasets\n",
    "source_medium = pd.merge(source_df, medium_df[['MEDIUM_IDEAL','MEDIUM_ACTUAL','MEDIUM_SIMILARITY_SCORE','UTM_SID']], how='outer', left_on=source_df['UTM_SID'], right_on=medium_df['UTM_SID'])\n",
    "del source_medium['key_0']\n",
    "\n",
    "source_medium['SOURCE_ACTUAL'] = source_medium['SOURCE_ACTUAL'].astype(str).apply(str.lower).replace('nan','na')\n",
    "source_medium['MEDIUM_ACTUAL'] = source_medium['MEDIUM_ACTUAL'].astype(str).apply(str.lower).replace('nan','na')\n",
    "\n",
    "# create mapping field to join with the ideal datasets for pulling in categories like channel and paid/organic\n",
    "source_medium['MAPPING'] = source_medium['SOURCE_IDEAL'].astype(str) + '_' + source_medium['MEDIUM_IDEAL'].astype(str)\n",
    "ideal['MAPPING'] = ideal['SOURCE_IDEAL'].astype(str) + '_' + ideal['MEDIUM_IDEAL'].astype(str)\n",
    "\n",
    "source_medium_categories = pd.merge(source_medium, ideal[['CHANNEL','ACQUISITION','BRAND','CONTENT','CRM','DISTRIBUTION','OTHER','UNDEFINED','ORGANIC_PAID']], how='left', left_on=source_medium['MAPPING'], right_on=ideal['MAPPING'])\n",
    "del source_medium_categories['key_0']\n",
    "\n",
    "final = source_medium_categories[source_medium_categories['UNDEFINED'].notnull()]\n",
    "final['UTM_SID'] = final[['UTM_SID_x', 'UTM_SID_y']].bfill(axis=1).iloc[:, 0]\n",
    "del final['UTM_SID_x']\n",
    "del final['UTM_SID_y']\n",
    "del final['MAPPING']\n",
    "\n",
    "final['UTM_SID'] = final['UTM_SID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963aff5c",
   "metadata": {},
   "source": [
    "### Upload data into Snowflake/ODIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5db3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.cursor().execute(\n",
    "    \"\"\" CREATE TABLE IF NOT EXISTS ANALYSIS_MARKETING.UTM_SIMILARITY_MAPPING\n",
    "      (\"SOURCE_IDEAL\" NVARCHAR(200),\n",
    "        \"SOURCE_ACTUAL\" NVARCHAR(200),\n",
    "        \"SOURCE_SIMILARITY_SCORE\" FLOAT,\n",
    "        \"MEDIUM_IDEAL\" NVARCHAR(200),\n",
    "        \"MEDIUM_ACTUAL\" NVARCHAR(200),\n",
    "        \"MEDIUM_SIMILARITY_SCORE\" FLOAT,\n",
    "        \"CHANNEL\" NVARCHAR(20),\n",
    "        \"ACQUISITION\" INTEGER,\n",
    "        \"BRAND\" INTEGER,\n",
    "        \"CONTENT\" INTEGER,\n",
    "        \"CRM\" INTEGER,\n",
    "        \"DISTRIBUTION\" INTEGER,\n",
    "        \"OTHER\" INTEGER,\n",
    "        \"UNDEFINED\" INTEGER,\n",
    "        \"ORGANIC_PAID\" NVARCHAR(10),\n",
    "        \"UTM_SID\" INTEGER,\n",
    "        PRIMARY KEY (\"UTM_SID\")); \"\"\")\n",
    "\n",
    "#ctx.cursor().execute(\"\"\" drop table ANALYSIS_MARKETING.UTM_SIMILARITY_MAPPING; \"\"\")\n",
    "ctx.cursor().execute(\"\"\" truncate ANALYSIS_MARKETING.UTM_SIMILARITY_MAPPING; \"\"\")\n",
    "\n",
    "\n",
    "#Define the table name, schema, and database you want to write to\n",
    "#Note: the table, schema, and database need to already exist in Snowflake\n",
    "\n",
    "table_name = \"UTM_SIMILARITY_MAPPING\"\n",
    "schema = 'ANALYSIS_MARKETING'\n",
    "database = 'SANDBOX'\n",
    "\n",
    "#Combine these using the function from the Snowflake connector\n",
    "\n",
    "write_pandas(\n",
    "            conn=ctx,\n",
    "            df=final,\n",
    "            table_name=table_name,\n",
    "            database=database,\n",
    "            schema=schema\n",
    "        )\n",
    "\n",
    "ctx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a423397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
